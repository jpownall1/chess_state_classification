# Chess assignment report

(Replace the square-bracketed text with your own text. *Leave everything else unchanged.* Note, the reports are parsed to check word limits, etc. Changing the format may cause the parsing to fail.)

## Feature Extraction (Max 200 Words)

[First, I computed the principal components (eigenvectors of the dataset as column vectors) of the training data's covariance matrix using my method 'myPCAEV'. This is then stored in a model with the label 'eigen_vectors' using the method process_training_data. The reduce_dimensions method is passed in this model, and I then use the matrix to transform the samples to having 10 dimensions (or features) by projecting those points onto the principal component axis, and this gives the 10 features that have the highest variance. This simplifies the datas features into fewer components to help the program fun faster, whilst preserving the data's spread.]

## Classifier (Max 200 Words)

[My classifier implements the Kth Nearest Neighbour Algorithm, and this algorithm is stored in the method 'my_KNN'. Its parameters are training data, the labels and a single test sample. It first uses a module from the numpy linear algebra module find all of the Euclidean distances from the test sample to every training sample. It then extracts the 10 closest samples and their indexes. It then uses these indexes to find the label for these training samples, and finds the most frequent label from these closest neighbours. In classify, this procedure is repeated on every test sample that has been passed into the classify method, and the predicted labels for each test sample are stored in an array. This array now contains all of the predicted labels for the test data, and this is returned from the classify method.
The KNN method uses 10 nearest neighbours instead of 5 as 10 had a higher noisy correct classification by 0.3%, but it also had a reduction for clean classification at -0.3%. As the difference is equal, I decided on 10 as I believe a programs ability to identify noisy data means more than identifying clean data.]

## Full Board Classification (Max 200 Words)

[In full board classification, I first perform the classification on all of the ordered squares passed into the method to obtain their predicted labels. I then separate this list into its own individual boards by splitting the data into equal subsets of 64. Then, in a loop, I check for any pawns, which are labels "p" or "P" on rows 1 or 8 (the first and last). If a pawn is found, classification is then applied on this single sample again, without the pawn data in the Kth nearest neighbour classification. This means the chess piece will be labelled as its next nearest neighbour. This is then replaced in its individual board, and this takes place for every board passed in. After all the boards have been checked modified, they are concatenated together to be a single array of labels once again, and this is returned. This caused a 0.1% increase in accuracy for the clean data. With a K value of 5 in my_KNN, it increased the accuracy of noisy by 0.1%.]

## Performance

My percentage correctness scores (to 1 decimal place) for the development data are as follows.

Clean data:

- Square mode: [98.0%]
- Board mode: [98.1%]

Noisy data:

- Square mode: [94.1%]
- Board mode: [94.1%]

## Other information (Optional, Max 100 words)

[My classifier used to implement the Nearest Neighbour Algorithm in my method, 'my_NN'. It used a cosine distance (based on the angle between a pair of feature vectors) rather than the Euclidean distance to find the closest labelled piece of training data to classify its test data's samples. The NN classification results were  were 97.6% (clean) and 91.8% (noisy) and the KNN results were 98.0% (clean) and 94.1% (noisy), therefore I switched from NN to KNN for classification. This may be because KNN takes into account more than one point to help classify the sample.]
